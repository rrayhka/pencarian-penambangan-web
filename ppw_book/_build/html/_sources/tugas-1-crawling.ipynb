{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctmIWTyIViJb"
   },
   "source": [
    "# Crawling Web Berita CNBC Indonesia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8MjcHNNV5nh"
   },
   "source": [
    "## Apa itu Crawling?\n",
    "\n",
    "Crawling adalah proses di mana mesin pencari mengunjungi halaman-halaman web untuk menemukan dan mengindeks konten. Dalam proses ini, bot atau robot yang dikenal sebagai \"crawler\" atau \"spider\" mengumpulkan informasi dari berbagai halaman web untuk memastikan bahwa konten terbaru dan relevan tersedia dalam indeks mesin pencari.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmT8Wmo8WDd_"
   },
   "source": [
    "## Proses Crawling\n",
    "\n",
    "Pada contoh ini, proses crawling dilakukan pada website CNBC Indonesia untuk mengumpulkan data berita. Artikel-artikel berita akan diambil dari beberapa kategori yang berbeda, seperti Research, News, Tech, dan Market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpM-0GTtWNY9"
   },
   "source": [
    "## Tool atau libray yang diperlukan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pOBXk9C1Sn97"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BI5sazE7WZu-"
   },
   "source": [
    "\n",
    "- **Requests** digunakan untuk mengambil konten HTML/HTTP dari sebuah website.\n",
    "- **BeautifulSoup** berfungsi untuk mengurai dan memproses data dari HTML/XML.\n",
    "- **Pandas** digunakan untuk menyimpan data dalam format yang mudah dibaca dan diproses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I50jpU9QWfwc"
   },
   "source": [
    "## Code Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ES9mNcl6Wmus"
   },
   "source": [
    "#### Inisialisasi Variabel dan URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mozJuDBQSv58"
   },
   "outputs": [],
   "source": [
    "judul = []\n",
    "tanggal = []\n",
    "isi = []\n",
    "url_list = []\n",
    "kategori_list = []\n",
    "\n",
    "base_urls = [\n",
    "    \"https://www.cnbcindonesia.com/research/indeks/127/\",\n",
    "    \"https://www.cnbcindonesia.com/news/indeks/3/\",\n",
    "    \"https://www.cnbcindonesia.com/tech/indeks/12/\",\n",
    "    \"https://www.cnbcindonesia.com/market/indeks/5/\"\n",
    "]\n",
    "categories = [\n",
    "    \"Research\",\n",
    "    \"News\",\n",
    "    \"Tech\",\n",
    "    \"Market\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwcq13poWzeB"
   },
   "source": [
    "\n",
    "Variabel-variabel ini digunakan untuk menyimpan data yang akan diambil dari website. `base_urls` adalah daftar URL yang akan dikunjungi untuk mengambil artikel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5YSAfcNW20J"
   },
   "source": [
    "#### Proses Pengambilan Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sPlhEuPKXJ1j"
   },
   "outputs": [],
   "source": [
    "payload = {'tipe': 'artikel'}\n",
    "\n",
    "for news, category in zip(base_urls, categories):\n",
    "    for page in range(1, 11):  # Mengambil dari halaman 1 hingga 10\n",
    "        url = f\"{news}{page}\"\n",
    "        response = requests.get(url, params=payload)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            articles = soup.find_all(\"article\")\n",
    "\n",
    "            # Menambahkan kategori ke dalam list\n",
    "            kategori = category  # Menggunakan kategori dari list `categories`\n",
    "\n",
    "            # Pada kode dibawah ini akan mengunjungi satu persatu artikel yang ada di halaman ke-n\n",
    "            for article in articles:\n",
    "                link = article.find(\"a\")[\"href\"]  # Untuk mendapatkan link\n",
    "                article_response = requests.get(link)\n",
    "                if article_response.status_code == 200:\n",
    "                    articleFull = BeautifulSoup(article_response.content, \"html.parser\")\n",
    "                    judulArtikel = articleFull.find(\n",
    "                        \"h1\", class_=\"mb-4 text-32 font-extrabold\"\n",
    "                    ).text.strip()  # Untuk mendapatkan judul artikel\n",
    "                    tanggalArtikel = articleFull.find(\n",
    "                        \"div\", class_=\"text-cm text-gray\"\n",
    "                    ).text.strip()  # Untuk mendapatkan tanggal artikel diterbitkan\n",
    "\n",
    "                    # Isi artikel terdapat pada tag div dengan class detail-text\n",
    "                    artikel_element = articleFull.find(\"div\", class_=\"detail-text\")\n",
    "                    # Mengambil semua isi artikel yang terdapat di tag p\n",
    "                    artikelTeks = [p.get_text(strip=True) for p in artikel_element.find_all(\"p\")]\n",
    "                    artikel_content = \"\\n\".join(artikelTeks)\n",
    "\n",
    "                    # Menambahkan judul, tanggal, isi, dan url ke dalam list yang sudah diinisialisasikan\n",
    "                    judul.append(judulArtikel)\n",
    "                    tanggal.append(tanggalArtikel)\n",
    "                    isi.append(artikel_content)\n",
    "                    url_list.append(link)\n",
    "                    kategori_list.append(kategori)\n",
    "                else:\n",
    "                    print(f\"Error: {article_response.status_code}\")\n",
    "        else:\n",
    "            print(f\"Error : {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgYzkLG-YgW8"
   },
   "source": [
    "Setiap artikel diambil dan data seperti judul, tanggal, dan isi artikel diproses dan disimpan dalam list yang telah disiapkan. Jika ada error dalam pengambilan data, status error akan diprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aySOWR3bRdS"
   },
   "source": [
    "##### Mengecek apakahh panjang hasil scrapping sama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IPAxh5E3TKeP",
    "outputId": "d227dcae-ea4b-4b07-dd82-25b7996d9a94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398 398 398 398 398\n"
     ]
    }
   ],
   "source": [
    "print(len(judul), len(tanggal), len(isi), len(url_list), len(kategori_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MKkRAcGbdjt"
   },
   "source": [
    "### Convert data ke dalam csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uM-_hSs-TN2M"
   },
   "outputs": [],
   "source": [
    "# Membuat dataframe dari list data\n",
    "df = pd.DataFrame({\"judul\": judul, \"tanggal\": tanggal, \"isi\": isi, \"url\": url_list, \"kategori\": kategori_list})\n",
    "\n",
    "# Menyimpan dataframe ke file csv\n",
    "df.to_csv(\"data-artikel-cnbc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwytvXGce22U"
   },
   "source": [
    "Setelah semua data dikumpulkan, data tersebut disimpan ke dalam sebuah dataframe menggunakan Pandas dan disimpan dalam format CSV untuk analisis lebih lanjut.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "QZs5Hy56TeKv",
    "outputId": "e8261eaa-f5fb-4524-a309-40ef1c1c2e6d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>judul</th>\n",
       "      <th>tanggal</th>\n",
       "      <th>isi</th>\n",
       "      <th>url</th>\n",
       "      <th>kategori</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bukan Kaleng-kaleng! Nilai Skuad Timnas Indone...</td>\n",
       "      <td>09 September 2024 19:30</td>\n",
       "      <td>Jakarta, CNBC Indonesia -Kedatangan pemain-pem...</td>\n",
       "      <td>https://www.cnbcindonesia.com/research/2024090...</td>\n",
       "      <td>Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aset Bank Mandiri Tembus Rp 2.200 T, Paling Ju...</td>\n",
       "      <td>09 September 2024 19:05</td>\n",
       "      <td>Jakarta, CNBC Indonesia -Sepanjang paruh perta...</td>\n",
       "      <td>https://www.cnbcindonesia.com/research/2024090...</td>\n",
       "      <td>Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Emas Meredup Saat Suku Bunga The Fed Mau Turun...</td>\n",
       "      <td>09 September 2024 18:10</td>\n",
       "      <td>Jakarta,CNBC Indonesia -Harga emas dunia melem...</td>\n",
       "      <td>https://www.cnbcindonesia.com/research/2024090...</td>\n",
       "      <td>Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tetangga RI Dihantam 'Bencana' Iklim, Harga Ba...</td>\n",
       "      <td>09 September 2024 15:55</td>\n",
       "      <td>Jakarta, CNBC Indonesia -Indeks Harga Konsumen...</td>\n",
       "      <td>https://www.cnbcindonesia.com/research/2024090...</td>\n",
       "      <td>Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tantama Hingga Perwira, Ini Urutan Lengkap Pan...</td>\n",
       "      <td>09 September 2024 14:35</td>\n",
       "      <td>Jakarta, CNBC Indonesia-Tentara Nasional Indon...</td>\n",
       "      <td>https://www.cnbcindonesia.com/research/2024090...</td>\n",
       "      <td>Research</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               judul                  tanggal  \\\n",
       "0  Bukan Kaleng-kaleng! Nilai Skuad Timnas Indone...  09 September 2024 19:30   \n",
       "1  Aset Bank Mandiri Tembus Rp 2.200 T, Paling Ju...  09 September 2024 19:05   \n",
       "2  Emas Meredup Saat Suku Bunga The Fed Mau Turun...  09 September 2024 18:10   \n",
       "3  Tetangga RI Dihantam 'Bencana' Iklim, Harga Ba...  09 September 2024 15:55   \n",
       "4  Tantama Hingga Perwira, Ini Urutan Lengkap Pan...  09 September 2024 14:35   \n",
       "\n",
       "                                                 isi  \\\n",
       "0  Jakarta, CNBC Indonesia -Kedatangan pemain-pem...   \n",
       "1  Jakarta, CNBC Indonesia -Sepanjang paruh perta...   \n",
       "2  Jakarta,CNBC Indonesia -Harga emas dunia melem...   \n",
       "3  Jakarta, CNBC Indonesia -Indeks Harga Konsumen...   \n",
       "4  Jakarta, CNBC Indonesia-Tentara Nasional Indon...   \n",
       "\n",
       "                                                 url  kategori  \n",
       "0  https://www.cnbcindonesia.com/research/2024090...  Research  \n",
       "1  https://www.cnbcindonesia.com/research/2024090...  Research  \n",
       "2  https://www.cnbcindonesia.com/research/2024090...  Research  \n",
       "3  https://www.cnbcindonesia.com/research/2024090...  Research  \n",
       "4  https://www.cnbcindonesia.com/research/2024090...  Research  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqYfCWHQe8S5"
   },
   "source": [
    "## Kesimpulan\n",
    "\n",
    "Dengan menggunakan requests, BeautifulSoup, dan Pandas, kita dapat melakukan web scraping untuk mengumpulkan data dari berbagai halaman web. Ini memungkinkan kita untuk menganalisis data dari website dengan lebih efisien."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
